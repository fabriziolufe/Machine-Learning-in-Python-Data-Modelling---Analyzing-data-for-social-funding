{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e85081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas               import DataFrame\n",
    "from pandas               import read_csv\n",
    "from pandas               import to_numeric\n",
    "from numpy                import array\n",
    "from numpy                import random\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics      import auc\n",
    "from sklearn.metrics      import roc_auc_score\n",
    "from sklearn.cluster      import KMeans\n",
    "from sklearn.cluster      import AgglomerativeClustering\n",
    "from matplotlib           import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from datetime             import datetime\n",
    "import import_ipynb\n",
    "from scipy.stats          import pearsonr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics      import auc\n",
    "from sklearn.metrics      import roc_auc_score\n",
    "from matplotlib           import pyplot\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree           import DecisionTreeClassifier\n",
    "from sklearn.ensemble       import RandomForestClassifier\n",
    "from sklearn.ensemble       import GradientBoostingClassifier\n",
    "from sklearn.svm            import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors      import KNeighborsClassifier\n",
    "from sklearn.metrics        import accuracy_score\n",
    "from sklearn.metrics        import roc_auc_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, ParameterGrid, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from datetime import timedelta\n",
    "from ieseg import partition\n",
    "from ieseg import roc\n",
    "from ieseg import cumulativeResponse\n",
    "from ieseg import cumulativeGains\n",
    "from ieseg import lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a72f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(Dataset, separation=';',lowMemory=True): \n",
    "    '''Read file into csv with a sep of \";\", proceeds to print related info for the file and returns the created dataframe'''\n",
    "    df= pd.read_csv(Dataset, sep=separation,low_memory=lowMemory)\n",
    "    print(df.info())\n",
    "    return df\n",
    "\n",
    "def my_mode(sample):\n",
    "    c = Counter(sample)\n",
    "    return [k for k, v in c.items() if v == c.most_common(1)[0][1]]\n",
    "\n",
    "# Function that interpret percentiles as RFM values\n",
    "    # By creating 5 groups\n",
    "\n",
    "def get_rfm_value(percentile_value):\n",
    "    \"\"\"Returns value 1-5 based on the percentile given\"\"\"\n",
    "    if percentile_value <= 0.2:\n",
    "        return 1\n",
    "    elif percentile_value <= 0.4:\n",
    "        return 2\n",
    "    elif percentile_value <= 0.6:\n",
    "        return 3\n",
    "    elif percentile_value <= 0.8:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "\n",
    "# Apply the Weighted Aggregation Approach to return overall RFM scores\n",
    "    # We assign a 20% weight for the Recency value (6 because 5*4 = 20)\n",
    "    # We assign a 30% weight for the Frequency value (4 because 5*6 = 30)\n",
    "    # We assign a 50% weight for the Monetary value (10 because 5*10 = 50)\n",
    "# In order to became its original value\n",
    "\n",
    "def get_weigthed_agg(df):\n",
    "    \"\"\"Returns the sum of scaled R, F and M values.\"\"\"\n",
    "    new_R = df['R']*4\n",
    "    new_F = df['F']*6\n",
    "    new_M = df['M']*10\n",
    "    return new_R + new_F + new_M\n",
    "\n",
    "def Creating_Variables(campaign_number,gifts,donors,campaign_df= DataFrame,campaign_date=str): \n",
    "    # Select donors to the campaign only (Target) - TRAINING\n",
    "    TARGET_donors = pd.merge(campaign_df, gifts.loc[gifts['campaignID'] == campaign_number,\n",
    "                                                      ['donorID', 'amount']], how = 'left', on = 'donorID')\n",
    "    # Replace 123,0 by 123.0 for the amount(format)\n",
    "    TARGET_donors['amount']=TARGET_donors['amount'].astype(str).str.replace(',','.').astype(float)\n",
    "    \n",
    "    # Calculate the target variable for the TRAIN data\n",
    "    TARGET_donors['amount']=np.where(TARGET_donors['amount'] >20, 1, 0)\n",
    "    #rename to donated\n",
    "    TARGET_donors = TARGET_donors.rename(columns ={\"amount\":\"donated\"})\n",
    "    print('Target variable created succesfully, you are awesome!')\n",
    "    \n",
    "    # Select only the gifts made before the date of the campaign\n",
    "    gifts['date']=pd.to_datetime(gifts['date'])\n",
    "    #filtering in our TRAINING time window...\n",
    "    gifts_ = gifts[gifts['date'] < campaign_date]\n",
    "    \n",
    "    # Aggregate the total amount of gift per donor (MONETARY) - TRAINING\n",
    "    # Total_amount for the last 5 years\n",
    "    campaign_date=pd.to_datetime(campaign_date)\n",
    "    gifts_mon = gifts[(gifts['date'] > (campaign_date-timedelta(days=1825)))&(gifts['date'] < campaign_date)]\n",
    "    total_amount = gifts_mon.groupby('donorID')['amount'].sum().rename('total_amount').reset_index()\n",
    "    print('dates filtered succesfully, you are awesome')\n",
    "    print('start_date of analysis: ',campaign_date-timedelta(days=1825))\n",
    "    print('final date of analysis: ',campaign_date)\n",
    "    print(\"Amount of donors that have donated in total more than 50,000EU (in 5 years): \",total_amount[total_amount['total_amount']>50000].shape[0])\n",
    "    #if (total_amount[total_amount['total_amount']>50000].shape[0]>0):\n",
    "        #print('Outliers, be careful') \n",
    "    #else \n",
    "        #print('No Outliers in MONETARY')\n",
    "    # Aggregate the last date of donation for each donor (RECENCY) - TRAINING\n",
    "    ##\n",
    "    last_donation = gifts_mon.groupby('donorID')['date'].agg(['min', 'max']).reset_index()\n",
    "    last_donation['LOR'] = last_donation['max'] - last_donation['min']\n",
    "    last_donation = last_donation.rename(columns={'min':'date_first_donation', 'max':'date_last_donation'})\n",
    "    last_donation['LOR'] = last_donation['LOR'].astype(str).str[:-5].astype(int)\n",
    "    last_donation.loc[last_donation['LOR'] == 0, 'LOR'] =  1\n",
    "    print('Number of donors: ',last_donation.shape[0])\n",
    "    # Calculate the number of days since the last transaction - TRAINING\n",
    "    last_donation['days_since_last_donation'] = pd.to_datetime(campaign_date,format = '%Y-%m-%d') - last_donation['date_last_donation']\n",
    "    last_donation['days_since_last_donation'] = last_donation['days_since_last_donation'].dt.days\n",
    "    ## Aggregate the number of gifts per donor - TRAINING\n",
    "    # Nbr of gifts for the last 5 years\n",
    "    frequency = gifts_mon.groupby(['donorID'])['amount'].count().rename('nbr_gifts').reset_index()\n",
    "    # Calculate the RFM scores based on the total donation - TRAINING\n",
    "    gifts_rfm = gifts_mon.groupby('donorID').agg({'date':'max', 'donorID':'count', 'amount':'sum'})\n",
    "    gifts_rfm = gifts_rfm.rename(columns={'date':'latest_date',\n",
    "                                             'donorID':'gifts_count',\n",
    "                                             'amount':'total_amount'})\n",
    "    # We calculate and assign RFM values to each client\n",
    "    # We calculate the percentiles to figure out what are the clients' value\n",
    "    percentiles = gifts_rfm.rank(pct=True, method='dense')\n",
    "    percentiles.head()\n",
    "    #apply our get rfm value\n",
    "    rfm_values = percentiles.applymap(get_rfm_value)\n",
    "    rfm_values.columns = ['R', 'F', 'M']\n",
    "    #Use our get_weighted_agg function to present clearer socres.\n",
    "    rfm_values['weighted_rfm_scores'] = rfm_values.apply(get_weigthed_agg, axis=1)\n",
    "    # Select only the donorID and the weighted_rfm_scores for later merge\n",
    "\n",
    "    rfm_values = rfm_values.reset_index()\n",
    "    rfm_values = rfm_values.loc[:,['donorID', 'weighted_rfm_scores']].copy(deep=True)\n",
    "\n",
    "    # Aggregate the average donation per campaign - TRAINING\n",
    "    ### Aggregate the average donation per campaign\n",
    "    avg_donation_per_campaign = gifts.groupby(['donorID']).agg({'amount':'sum','campaignID':'count'}).reset_index()\n",
    "    avg_donation_per_campaign\n",
    "    #creating the average in column\n",
    "    avg_donation_per_campaign['avg_amount_per_campaign'] = np.where(avg_donation_per_campaign['campaignID']>0,\n",
    "                                                                    round(avg_donation_per_campaign['amount']/avg_donation_per_campaign['campaignID'],2),\n",
    "                                                                    avg_donation_per_campaign['amount'])\n",
    "\n",
    "\n",
    "    #Average donation per campaign - fixing column names\n",
    "    avg_donation_per_campaign = avg_donation_per_campaign.rename(columns={'campaignID':'nbr_campaign_donation'})\n",
    "    avg_donation_per_campaign = avg_donation_per_campaign[['donorID', 'nbr_campaign_donation','avg_amount_per_campaign']]\n",
    "    avg_donation_per_campaign\n",
    "    # Aggregate the average donation per donor - TRAINING\n",
    "    avg_donation_per_donor = gifts.groupby('donorID')['amount'].mean().rename('avg_donation').reset_index()\n",
    "    avg_donation_per_donor['avg_donation'] = round(avg_donation_per_donor['avg_donation'],2)\n",
    "    \n",
    "    #extracting the year.\n",
    "    donors['dateOfBirth']=pd.to_datetime(donors['dateOfBirth'])\n",
    "    donors['yearOfBirth']=donors['dateOfBirth'].astype(str).str[:4].astype(int)\n",
    "    donors['yearOfBirth']\n",
    "    donors_campaign= donors.copy(deep=True)\n",
    "\n",
    "    #generating the age...\n",
    "    donors_campaign['age'] = campaign_date.year - donors_campaign['yearOfBirth']\n",
    "    print(\"year of the campaign: \",campaign_date.year)\n",
    "    # life expectancy = https://www.healthybelgium.be/en/health-status/life-expectancy-and-quality-of-life/life-expectancy\n",
    "    donors_campaign=donors_campaign[donors_campaign['age']<82]\n",
    "\n",
    "    #Creating a dummy for people under 30 or over 30 \n",
    "    donors_campaign['age_dummy_30']= np.where(donors_campaign['age']< 30, 0, 1)\n",
    "    \n",
    "    \n",
    "    merged1 = pd.merge(TARGET_donors, total_amount, how = 'left', on = 'donorID')\n",
    "    merged2 = pd.merge(merged1, last_donation, how = 'left', on = 'donorID')\n",
    "    merged3 = pd.merge(merged2, frequency, how = 'left', on = 'donorID')\n",
    "    merged4 = pd.merge(merged3, rfm_values, how = 'left', on = 'donorID')\n",
    "    merged5 = pd.merge(merged4, avg_donation_per_campaign, how = 'left', on = 'donorID')\n",
    "    merged6 = pd.merge(merged5, avg_donation_per_donor, how = 'left', on = 'donorID')\n",
    "    merged_final = pd.merge(merged6, donors_campaign, how = 'left', on = 'donorID').drop_duplicates(subset='donorID',keep='last')\n",
    "    print('variable creation DONE')\n",
    "    #final table.\n",
    "    return merged_final\n",
    "\n",
    "def Fixing_Nan(df,donors):\n",
    "    \n",
    "    df=df.dropna(how='all')\n",
    "\n",
    "    fixed_df=df[df['donorID'].isin(donors['donorID'])]\n",
    "\n",
    "    #taking only people in the campaign that are present in our donors database\n",
    "    print(\"before filter: \",df.shape[0])\n",
    "    print(\"after filter: \",fixed_df.shape[0])\n",
    "\n",
    "    #flag variable for NaN\n",
    "    fixed_df.loc[:,'not_donated_in_the_past'] = np.where(fixed_df.loc[:,'LOR'].isna(), 1, 0)\n",
    "    fixed_df=fixed_df.drop(['date_first_donation', 'date_last_donation','dateOfBirth'], axis=1)\n",
    "    fixed_df=fixed_df.fillna(0)\n",
    "    fixed_df=fixed_df.drop('yearOfBirth',axis=1)\n",
    "    \n",
    "    \n",
    "    return fixed_df\n",
    "\n",
    "def Feature_Engineering(df, test_df): \n",
    "    variables=df.drop('donated',axis=1)\n",
    "    listPC=[]\n",
    "    for (columnName, columnData) in variables.iteritems():\n",
    "        print('Column Name : ', columnName, '///Correlation and P value: ',pearsonr(variables[columnName],df['donated']))\n",
    "    for (columnName, columnData) in variables.iteritems():\n",
    "        if pearsonr(variables[columnName],df['donated'])[1]<0.001:\n",
    "            print(\"Use this variable: \",columnName)\n",
    "            listPC.append(columnName)\n",
    "    #Creating Pipeline (Transforming dataframe for ML Algorithm)\n",
    "    NumFeat=variables.drop(['donorID'],axis=1)#[listPC]\n",
    "    print(\"Standardizing the following variables: \",listPC)\n",
    "    standard_scaler = StandardScaler()  \n",
    "    fitted = standard_scaler.fit(df[listPC])  \n",
    "    transformed_df = pd.DataFrame(fitted.transform(df[listPC]),columns=listPC)\n",
    "    transformed_df=transformed_df.join(df[['donorID','donated']].reset_index().drop('index',axis=1))\n",
    "    \n",
    "    transformed_df_test = pd.DataFrame(fitted.transform(test_df[listPC]),columns=listPC)\n",
    "    transformed_df_test=transformed_df_test.join(test_df[['donorID','donated']].reset_index().drop('index',axis=1))\n",
    "    print(\"standarization FINISHED SUCCESFULLY\")\n",
    "    \n",
    "    \n",
    "    return transformed_df,transformed_df_test,listPC\n",
    "\n",
    "def stepwiseRegresion (model,trainingSet: DataFrame, testSet: DataFrame, selectedFeatures: [str], target: [str]) -> DataFrame:\n",
    "\n",
    " \n",
    "\n",
    "    def computeAUC (forFeatures: [str]) -> ([str],float,float):\n",
    "    \n",
    "        model.fit(trainingSet[forFeatures], trainingSet[target])\n",
    "\n",
    " \n",
    "\n",
    "        trainingSet[\"proba churn stepwise\"] = DataFrame(model.predict_proba(trainingSet[forFeatures]))[1]\n",
    "        testSet[\"proba churn stepwise\"]     = DataFrame(model.predict_proba(testSet[forFeatures]))[1]\n",
    "\n",
    " \n",
    "\n",
    "        aucTraining = roc_auc_score(array(trainingSet[target]),array(trainingSet[\"proba churn stepwise\"]))\n",
    "        aucTest     = roc_auc_score(array(testSet[target]),array(testSet[\"proba churn stepwise\"]))\n",
    "\n",
    " \n",
    "\n",
    "        trainingSet.drop(\"proba churn stepwise\", axis = 1)\n",
    "        testSet.drop(\"proba churn stepwise\", axis = 1)\n",
    "\n",
    " \n",
    "\n",
    "        return (forFeatures,aucTraining,aucTest)\n",
    "\n",
    " \n",
    "\n",
    "    featuresOrder = []\n",
    "    forwardSelection = []\n",
    "\n",
    " \n",
    "\n",
    "    for step in range(len(selectedFeatures)):\n",
    "        print(f\"step {step+1}\")\n",
    "        aucs = []\n",
    "        for feature in selectedFeatures:\n",
    "            if feature not in featuresOrder:\n",
    "                modelFeatures = featuresOrder.copy()\n",
    "                modelFeatures.append(feature)\n",
    "                aucs.append(computeAUC(forFeatures = modelFeatures))\n",
    "\n",
    " \n",
    "\n",
    "        steps = DataFrame(aucs)\n",
    "        steps.columns = [\"Feature\",\"AUC training\", \"AUC test\"]\n",
    "        steps = steps.sort_values(by=[\"AUC test\"], ascending = False)\n",
    "\n",
    " \n",
    "\n",
    "        featuresOrder = steps[\"Feature\"].iloc[0]\n",
    "        forwardSelection.append((step+1, steps[\"Feature\"].iloc[0],steps[\"AUC training\"].iloc[0],steps[\"AUC test\"].iloc[0]))\n",
    "\n",
    " \n",
    "\n",
    "    df = DataFrame(forwardSelection)\n",
    "    df.columns = (\"Step\",\"Features\",\"AUC Train\",\"AUC Test\")\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
